---
title: "QFORMER: An Efficient Foundational PDE Model for the Time-Dependent
Schr ̈odinger Equation"
authors:
- admin
date: "2025-01-04T00:00:00Z"
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: "2024-12-10T00:00:00Z"

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ['Research Proposal']

# Publication name and optional abbreviated publication name.
publication: ""
publication_short: ""

abstract: The QFORMER project aims to develop a novel Transformer-based deep learning model for solving the time-dependent Schrödinger equation with high accuracy and efficiency. Building on advances in multiscale operator transformers (MOTs) and physics-informed neural networks (PINNs), QFORMER integrates elements from POSEIDON and PINNsFormer to capture both temporal dependencies and multiscale quantum phenomena. Unlike previous Transformer-based approaches, which focus on steady-state solutions, QFORMER is designed to generalize across arbitrary electron configurations and initial states, significantly enhancing its applicability in quantum chemistry, condensed matter physics, and materials science. The model is pretrained on analytically solvable quantum systems, including the harmonic oscillator with time-dependent frequency, the Rabi model, and the Landau-Zener problem, ensuring robust performance across diverse Hamiltonians. By leveraging the semi-group property of the Schrödinger equation, QFORMER aims to reduce computational costs and improve scalability, offering a potential breakthrough in quantum mechanical simulations. If successful, this model could revolutionize computational quantum mechanics by providing an efficient, scalable, and generalizable deep learning framework for solving complex quantum systems.

# Summary. An optional shortened abstract.
summary: QFORMER is a proposed Transformer-based deep learning model designed to solve the time-dependent Schrödinger equation by integrating multiscale operator transformers and physics-informed neural networks, aiming for scalable and accurate quantum simulations across diverse electron configurations and initial states.

tags:
- Scientific Machine Learning | Foundation Models for PDEs

featured: true

# links:
# - name: Custom Link
# url: http://example.org
# url_code: ''
# url_dataset: '#'
# url_poster: '#'
# url_project: ''
# url_slides: ''
# url_source: '#'
# url_video: '#'

url_pdf: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ''
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
- internal-project

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: example
---

<!-- {{% callout note %}}
Create your slides in Markdown - click the *Slides* button to check out the example.
{{% /callout %}}

Add the publication's **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). -->
